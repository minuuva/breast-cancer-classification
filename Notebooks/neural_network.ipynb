{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "826cd001",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTS\n",
    "\n",
    "# PyTorch - neural network framework\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "\n",
    "# data manipulation and analysis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# preprocessing and model selection\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# evaluation metrics\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    roc_auc_score,\n",
    "    confusion_matrix,\n",
    "    classification_report,\n",
    "    roc_curve,\n",
    "    auc\n",
    ")\n",
    "\n",
    "# warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d3bd84b",
   "metadata": {},
   "source": [
    "#### 1. Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "26e080a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Dimensions:\n",
      "X_train: (455, 30)\n",
      "y_train: (455,)\n",
      "------------------------------------------------------------\n",
      "Target Distribution:\n",
      "Benign (0): 285 samples (62.6%)\n",
      "Malignant (1): 170 samples (37.4%)\n",
      "Total: 455 samples\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Load training dataset\n",
    "train_df = pd.read_csv('../Data/breast_cancer_trainset.csv')\n",
    "\n",
    "X_train = train_df.drop(columns=['diagnosis'])\n",
    "y_train = train_df['diagnosis']\n",
    "\n",
    "# Data dimensions\n",
    "print(\"Data Dimensions:\")\n",
    "print(f\"X_train: {X_train.shape}\")\n",
    "print(f\"y_train: {y_train.shape}\")\n",
    "\n",
    "# Target distribution\n",
    "print(\"-\"*60)\n",
    "print(\"Target Distribution:\")\n",
    "print(f\"Benign (0): {(y_train==0).sum()} samples ({(y_train==0).sum()/len(y_train)*100:.1f}%)\")\n",
    "print(f\"Malignant (1): {(y_train==1).sum()} samples ({(y_train==1).sum()/len(y_train)*100:.1f}%)\")\n",
    "print(f\"Total: {len(y_train)} samples\")\n",
    "print(\"-\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22c68737",
   "metadata": {},
   "source": [
    "#### 2. Preprocessing\n",
    "\n",
    "Since sklearn pipelines don't work directly with PyTorch, we implement preprocessing carefully to avoid data leakage. We fit the StandardScaler on training data and convert the data to PyTorch tensors. For cross-validation, we'll fit a NEW scaler for each fold to maintain proper data handling and prevent leakage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "aac69bf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing Complete:\n",
      "Training samples: 455\n",
      "Batch size: 32\n",
      "Number of training batches: 15\n",
      "Feature shape: 30 features\n"
     ]
    }
   ],
   "source": [
    "# Initialize StandardScaler (neural networks need normalized features)\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit and transform training data\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train_tensor = torch.FloatTensor(X_train_scaled)\n",
    "y_train_tensor = torch.LongTensor(y_train.values)\n",
    "\n",
    "# Create TensorDataset\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "\n",
    "# Create DataLoader for batching\n",
    "batch_size = 32  # Default batch size for baseline\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "print(\"Preprocessing Complete:\")\n",
    "print(f\"Training samples: {len(train_dataset)}\")\n",
    "print(f\"Batch size: {batch_size}\")\n",
    "print(f\"Number of training batches: {len(train_loader)}\")\n",
    "print(f\"Feature shape: {X_train_scaled.shape[1]} features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "334928db",
   "metadata": {},
   "source": [
    "#### 3. Neutral Network Architecture\n",
    "\n",
    "We define a flexible feedforward neural network architecture using PyTorch's nn.Module class. The network consists of an input layer (30 features), two hidden layers with ReLU activation functions, and an output layer for binary classification (2 classes: benign or malignant). Each hidden layer includes batch normalization to stabilize training and dropout for regularization to prevent overfitting. The architecture is parameterized, allowing us to easily tune hyperparameters like hidden layer sizes (default: 64 and 32 neurons) and dropout rate (default: 0.3) during the optimization phase. Batch normalization normalizes the inputs to each layer, which helps the model train faster and more reliably, while dropout randomly deactivates a proportion of neurons during training to encourage the network to learn robust features rather than memorizing the training data. This architecture balances model complexity with generalization capability, making it suitable for our relatively small dataset of 455 training samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1997dbdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Architecture:\n",
      "BreastCancerNet(\n",
      "  (fc1): Linear(in_features=30, out_features=64, bias=True)\n",
      "  (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu1): ReLU()\n",
      "  (dropout1): Dropout(p=0.3, inplace=False)\n",
      "  (fc2): Linear(in_features=64, out_features=32, bias=True)\n",
      "  (bn2): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu2): ReLU()\n",
      "  (dropout2): Dropout(p=0.3, inplace=False)\n",
      "  (fc3): Linear(in_features=32, out_features=2, bias=True)\n",
      ")\n",
      "Total parameters: 4322\n"
     ]
    }
   ],
   "source": [
    "# defining the Neural Network Architecture\n",
    "class BreastCancerNet(nn.Module):\n",
    "    def __init__(self, input_size=30, hidden_size1=64, hidden_size2=32, dropout_rate=0.3, num_classes=2):\n",
    "        \"\"\"\n",
    "        Nerual Network Architecture for Breast Cancer Classification\n",
    "        Args:\n",
    "            input_size: number of input features (30 for breast cancer dataset)\n",
    "            hidden_size1: number of neurons in the first hidden layer\n",
    "            hidden_size2: number of neurons in the second hidden layer\n",
    "            dropout_rate: dropout probability for regularization\n",
    "            num_classes: number of output classes (2 for binary classification)\n",
    "        \"\"\"\n",
    "        super(BreastCancerNet, self).__init__()\n",
    "\n",
    "        # first hidden layer\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size1)\n",
    "        self.bn1 = nn.BatchNorm1d(hidden_size1)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.dropout1 = nn.Dropout(dropout_rate)\n",
    "\n",
    "        # second hidden layer\n",
    "        self.fc2 = nn.Linear(hidden_size1, hidden_size2)\n",
    "        self.bn2 = nn.BatchNorm1d(hidden_size2)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.dropout2 = nn.Dropout(dropout_rate)\n",
    "\n",
    "        # output layer\n",
    "        self.fc3 = nn.Linear(hidden_size2, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass through the network\n",
    "        \"\"\"\n",
    "        # layer 1\n",
    "        x = self.fc1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.dropout1(x)\n",
    "        \n",
    "        # layer 2\n",
    "        x = self.fc2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.dropout2(x)\n",
    "        \n",
    "        # output layer\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# displaying the model architecture\n",
    "baseline_model = BreastCancerNet()\n",
    "print(\"Model Architecture:\")\n",
    "print(baseline_model)\n",
    "print(f\"Total parameters: {sum(p.numel() for p in baseline_model.parameters())}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "518cbe0b",
   "metadata": {},
   "source": [
    "#### 4. Training and Evaluation Functions\n",
    "\n",
    "We define reusable training and evaluation functions that will be used throughout our experimentation. The training function performs forward propagation, calculates loss using CrossEntropyLoss (which works for both binary and multi-class classification by treating our problem as a 2-class classification task), performs backpropagation, and updates model weights using the Adam optimizer. It tracks the average loss per epoch to monitor training progress. The evaluation function puts the model in evaluation mode (disabling dropout and batch normalization updates), makes predictions on the validation set without computing gradients (for efficiency), and calculates comprehensive performance metrics including accuracy, precision, recall, F1-score, and ROC-AUC. These functions encapsulate the core training loop logic, making our code modular and reusable for both baseline model training and later cross-validation experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "49887e29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and evaluation functions defined successfully!\n"
     ]
    }
   ],
   "source": [
    "# Training Function\n",
    "def train_model(model, train_loader, criterion, optimizer, num_epochs=50):\n",
    "    \"\"\"\n",
    "    Training loop for the neural network\n",
    "    Args:\n",
    "        model: Neural Network model to train\n",
    "        train_loader: DataLoader for training data\n",
    "        criterion: Loss function\n",
    "        optimizer: Optimization algorithm\n",
    "        num_epochs: Number of training epochs\n",
    "\n",
    "    Returns:\n",
    "        train_losses: list of average training loss per epoch\n",
    "    \"\"\"\n",
    "\n",
    "    train_losses = []\n",
    "\n",
    "    # looping over the number of epochs\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train() # setting model to training mode (enables dropout and batch normalization)\n",
    "        running_loss = 0.0\n",
    "\n",
    "        # iterating through the batches\n",
    "        for inputs, labels in train_loader:\n",
    "            # zeroing the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "            # forward pass: compute predictions\n",
    "            outputs = model(inputs)\n",
    "            # compute loss\n",
    "            loss = criterion(outputs, labels)\n",
    "            # backward pass: compute gradients\n",
    "            loss.backward()\n",
    "            # update weights\n",
    "            optimizer.step()\n",
    "\n",
    "            # accumulating the loss\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        # calculating the average loss for the epoch\n",
    "        avg_loss = running_loss / len(train_loader)\n",
    "        train_losses.append(avg_loss)\n",
    "\n",
    "        # printing progress every 5 epochs\n",
    "        if (epoch + 1) % 5 == 0:\n",
    "            print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    return train_losses\n",
    "\n",
    "# Evaluation Function\n",
    "def evaluate_model(model, val_loader, val_tensor_X, val_tensor_y):\n",
    "    \"\"\"\n",
    "    Evaluate model performance on validation set\n",
    "    Args:\n",
    "        model: Trained neural network model\n",
    "        val_loader: DataLoader for validation data\n",
    "        val_tensor_X: Validation features tensor\n",
    "        val_tensor_y: Validation labels tensor\n",
    "\n",
    "    Returns:\n",
    "        Dictionary containing all evaluation metrics\n",
    "    \"\"\"\n",
    "    model.eval()  # Set model to evaluation mode (disables dropout, batch norm training)\n",
    "    \n",
    "    # Make predictions without computing gradients (saves memory and computation)\n",
    "    with torch.no_grad():\n",
    "        # Get predictions\n",
    "        outputs = model(val_tensor_X)\n",
    "        _, predicted = torch.max(outputs, 1)  # Get class with highest score\n",
    "        \n",
    "        # Get prediction probabilities for ROC-AUC\n",
    "        probabilities = torch.softmax(outputs, dim=1)[:, 1]  # Probability of class 1 (malignant)\n",
    "        \n",
    "        # Convert to numpy for sklearn metrics\n",
    "        y_true = val_tensor_y.cpu().numpy()\n",
    "        y_pred = predicted.cpu().numpy()\n",
    "        y_prob = probabilities.cpu().numpy()\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred)\n",
    "    recall = recall_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "    roc_auc = roc_auc_score(y_true, y_prob)\n",
    "    \n",
    "    # Return all metrics as dictionary\n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'roc_auc': roc_auc,\n",
    "        'y_true': y_true,\n",
    "        'y_pred': y_pred,\n",
    "        'y_prob': y_prob\n",
    "    }\n",
    "\n",
    "print(\"Training and evaluation functions defined successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "677641c0",
   "metadata": {},
   "source": [
    "#### 5. Baseline Model Training\n",
    "\n",
    "We train a baseline neural network model with default hyperparameters. This baseline uses 50 training epochs, a learning rate of 0.001, and the Adam optimizer. We track the training loss over epochs to monitor convergence. Since we're using cross-validation for hyperparameter tuning, we don't evaluate the baseline separatelyâ€”the cross-validation process will determine optimal parameters across the entire training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2144eb7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline Model Training\n",
      "============================================================\n",
      "Epochs: 50\n",
      "Learning Rate: 0.001\n",
      "Optimizer: Adam\n",
      "Batch Size: 32\n",
      "============================================================\n",
      "\n",
      "Epoch 5/50, Loss: 0.2692\n",
      "Epoch 10/50, Loss: 0.1421\n",
      "Epoch 15/50, Loss: 0.1025\n",
      "Epoch 20/50, Loss: 0.1041\n",
      "Epoch 25/50, Loss: 0.0999\n",
      "Epoch 30/50, Loss: 0.0685\n",
      "Epoch 35/50, Loss: 0.0710\n",
      "Epoch 40/50, Loss: 0.0600\n",
      "Epoch 45/50, Loss: 0.0484\n",
      "Epoch 50/50, Loss: 0.0785\n",
      "\n",
      "Training Complete\n"
     ]
    }
   ],
   "source": [
    "# initializing baseline model\n",
    "baseline_model = BreastCancerNet(\n",
    "    input_size=X_train.shape[1],\n",
    "    hidden_size1=64,\n",
    "    hidden_size2=32,\n",
    "    dropout_rate=0.3,\n",
    "    num_classes=2\n",
    ")\n",
    "\n",
    "# defining the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(baseline_model.parameters(), lr=0.001)\n",
    "\n",
    "# training parameters\n",
    "num_epochs = 50\n",
    "\n",
    "print(\"Baseline Model Training\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Epochs: {num_epochs}\")\n",
    "print(f\"Learning Rate: 0.001\")\n",
    "print(f\"Optimizer: Adam\")\n",
    "print(f\"Batch Size: 32\")\n",
    "print(\"=\" * 60)\n",
    "print()\n",
    "\n",
    "# training the model\n",
    "baseline_train_losses = train_model(\n",
    "    model=baseline_model,\n",
    "    train_loader=train_loader,\n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer,\n",
    "    num_epochs=num_epochs\n",
    ")\n",
    "\n",
    "print()\n",
    "print(\"Training Complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3a7adcd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "BASELINE MODEL TRAINING COMPLETE\n",
      "============================================================\n",
      "Final Training Loss: 0.0785\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Plot Training Loss Curve\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, len(baseline_train_losses) + 1), baseline_train_losses, 'b-', linewidth=2)\n",
    "plt.xlabel('Epoch', fontsize=12)\n",
    "plt.ylabel('Training Loss', fontsize=12)\n",
    "plt.title('Baseline Model - Training Loss Over Epochs', fontsize=14, fontweight='bold')\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"BASELINE MODEL TRAINING COMPLETE\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Final Training Loss: {baseline_train_losses[-1]:.4f}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22ef5b6f",
   "metadata": {},
   "source": [
    "#### 6. Cross-Validation with Hyperparameter Tuning\n",
    "\n",
    "To complete the preprocessing-in-pipeline and find optimal hyperparameters, we implement manual k-fold cross-validation with StratifiedKFold (5 folds) to maintain class distribution across folds. In each fold iteration, 4 folds are used for training and 1 fold is held out for evaluation. Critically, for each fold, we fit a NEW StandardScaler on the training portion only and then transform both the training and held-out portions, preventing data leakage while embedding preprocessing within the cross-validation loop. We perform a grid search over multiple hyperparameters including learning rate, batch size, hidden layer architectures, and dropout rates. Each training run incorporates early stopping with a patience of 10 epochsâ€”training halts automatically when the held-out fold's recall stops improving, preventing overfitting and reducing computation time. For each hyperparameter combination, we train the model on each fold and track the average recall score (our primary metric for medical diagnosis to minimize false negatives). This process tests 243 configurations across 5 folds (1,215 training runs with early stopping) to identify the best-performing hyperparameters, which we'll use to train our final optimized model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a47dc76b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "HYPERPARAMETER TUNING WITH CROSS-VALIDATION (EARLY STOPPING)\n",
      "======================================================================\n",
      "Parameters to tune:\n",
      "  learning_rate: [0.001, 0.01, 0.1]\n",
      "  batch_size: [16, 32, 64]\n",
      "  hidden_size1: [64, 128, 256]\n",
      "  hidden_size2: [32, 64, 128]\n",
      "  dropout_rate: [0.2, 0.3, 0.5]\n",
      "\n",
      "Early Stopping: patience=10, max_epochs=100\n",
      "Total combinations: 243 = 243\n",
      "K-folds: 5\n",
      "Total training runs: 243 x 5 = 1215 (with early stopping)\n",
      "======================================================================\n",
      "\n",
      "[1/243] lr=0.001, batch=16, h1=64, h2=32, dropout=0.2\n",
      "  -> Recall: 0.9588 (avg 4 epochs)\n",
      "[2/243] lr=0.001, batch=16, h1=64, h2=32, dropout=0.3\n",
      "  -> Recall: 0.9471 (avg 6 epochs)\n",
      "[3/243] lr=0.001, batch=16, h1=64, h2=32, dropout=0.5\n",
      "  -> Recall: 0.9588 (avg 5 epochs)\n",
      "[4/243] lr=0.001, batch=16, h1=64, h2=64, dropout=0.2\n",
      "  -> Recall: 0.9647 (avg 5 epochs)\n",
      "[5/243] lr=0.001, batch=16, h1=64, h2=64, dropout=0.3\n",
      "  -> Recall: 0.9647 (avg 6 epochs)\n",
      "[6/243] lr=0.001, batch=16, h1=64, h2=64, dropout=0.5\n",
      "  -> Recall: 0.9647 (avg 8 epochs)\n",
      "[7/243] lr=0.001, batch=16, h1=64, h2=128, dropout=0.2\n",
      "  -> Recall: 0.9706 (avg 8 epochs)\n",
      "[8/243] lr=0.001, batch=16, h1=64, h2=128, dropout=0.3\n",
      "  -> Recall: 0.9588 (avg 5 epochs)\n",
      "[9/243] lr=0.001, batch=16, h1=64, h2=128, dropout=0.5\n",
      "  -> Recall: 0.9588 (avg 6 epochs)\n",
      "[10/243] lr=0.001, batch=16, h1=128, h2=32, dropout=0.2\n",
      "  -> Recall: 0.9824 (avg 8 epochs)\n",
      "[11/243] lr=0.001, batch=16, h1=128, h2=32, dropout=0.3\n",
      "  -> Recall: 0.9588 (avg 3 epochs)\n",
      "[12/243] lr=0.001, batch=16, h1=128, h2=32, dropout=0.5\n",
      "  -> Recall: 0.9647 (avg 7 epochs)\n",
      "[13/243] lr=0.001, batch=16, h1=128, h2=64, dropout=0.2\n",
      "  -> Recall: 0.9824 (avg 7 epochs)\n",
      "[14/243] lr=0.001, batch=16, h1=128, h2=64, dropout=0.3\n",
      "  -> Recall: 0.9706 (avg 8 epochs)\n",
      "[15/243] lr=0.001, batch=16, h1=128, h2=64, dropout=0.5\n",
      "  -> Recall: 0.9647 (avg 8 epochs)\n",
      "[16/243] lr=0.001, batch=16, h1=128, h2=128, dropout=0.2\n",
      "  -> Recall: 0.9824 (avg 4 epochs)\n",
      "[17/243] lr=0.001, batch=16, h1=128, h2=128, dropout=0.3\n",
      "  -> Recall: 0.9647 (avg 4 epochs)\n",
      "[18/243] lr=0.001, batch=16, h1=128, h2=128, dropout=0.5\n",
      "  -> Recall: 0.9588 (avg 5 epochs)\n",
      "[19/243] lr=0.001, batch=16, h1=256, h2=32, dropout=0.2\n",
      "  -> Recall: 0.9765 (avg 4 epochs)\n",
      "[20/243] lr=0.001, batch=16, h1=256, h2=32, dropout=0.3\n",
      "  -> Recall: 0.9765 (avg 8 epochs)\n",
      "[21/243] lr=0.001, batch=16, h1=256, h2=32, dropout=0.5\n",
      "  -> Recall: 0.9647 (avg 4 epochs)\n",
      "[22/243] lr=0.001, batch=16, h1=256, h2=64, dropout=0.2\n",
      "  -> Recall: 0.9647 (avg 3 epochs)\n",
      "[23/243] lr=0.001, batch=16, h1=256, h2=64, dropout=0.3\n",
      "  -> Recall: 0.9765 (avg 3 epochs)\n",
      "[24/243] lr=0.001, batch=16, h1=256, h2=64, dropout=0.5\n",
      "  -> Recall: 0.9706 (avg 5 epochs)\n",
      "[25/243] lr=0.001, batch=16, h1=256, h2=128, dropout=0.2\n",
      "  -> Recall: 0.9706 (avg 3 epochs)\n",
      "[26/243] lr=0.001, batch=16, h1=256, h2=128, dropout=0.3\n",
      "  -> Recall: 0.9647 (avg 3 epochs)\n",
      "[27/243] lr=0.001, batch=16, h1=256, h2=128, dropout=0.5\n",
      "  -> Recall: 0.9706 (avg 6 epochs)\n",
      "[28/243] lr=0.001, batch=32, h1=64, h2=32, dropout=0.2\n",
      "  -> Recall: 0.9647 (avg 1 epochs)\n",
      "[29/243] lr=0.001, batch=32, h1=64, h2=32, dropout=0.3\n",
      "  -> Recall: 0.9824 (avg 6 epochs)\n",
      "[30/243] lr=0.001, batch=32, h1=64, h2=32, dropout=0.5\n",
      "  -> Recall: 0.9647 (avg 13 epochs)\n",
      "[31/243] lr=0.001, batch=32, h1=64, h2=64, dropout=0.2\n",
      "  -> Recall: 0.9529 (avg 7 epochs)\n",
      "[32/243] lr=0.001, batch=32, h1=64, h2=64, dropout=0.3\n",
      "  -> Recall: 0.9647 (avg 6 epochs)\n",
      "[33/243] lr=0.001, batch=32, h1=64, h2=64, dropout=0.5\n",
      "  -> Recall: 0.9647 (avg 5 epochs)\n",
      "[34/243] lr=0.001, batch=32, h1=64, h2=128, dropout=0.2\n",
      "  -> Recall: 0.9706 (avg 6 epochs)\n",
      "[35/243] lr=0.001, batch=32, h1=64, h2=128, dropout=0.3\n",
      "  -> Recall: 0.9647 (avg 7 epochs)\n",
      "[36/243] lr=0.001, batch=32, h1=64, h2=128, dropout=0.5\n",
      "  -> Recall: 0.9529 (avg 5 epochs)\n",
      "[37/243] lr=0.001, batch=32, h1=128, h2=32, dropout=0.2\n",
      "  -> Recall: 0.9588 (avg 2 epochs)\n",
      "[38/243] lr=0.001, batch=32, h1=128, h2=32, dropout=0.3\n",
      "  -> Recall: 0.9588 (avg 5 epochs)\n",
      "[39/243] lr=0.001, batch=32, h1=128, h2=32, dropout=0.5\n",
      "  -> Recall: 0.9647 (avg 5 epochs)\n",
      "[40/243] lr=0.001, batch=32, h1=128, h2=64, dropout=0.2\n",
      "  -> Recall: 0.9647 (avg 6 epochs)\n",
      "[41/243] lr=0.001, batch=32, h1=128, h2=64, dropout=0.3\n",
      "  -> Recall: 0.9647 (avg 5 epochs)\n",
      "[42/243] lr=0.001, batch=32, h1=128, h2=64, dropout=0.5\n",
      "  -> Recall: 0.9647 (avg 5 epochs)\n",
      "[43/243] lr=0.001, batch=32, h1=128, h2=128, dropout=0.2\n",
      "  -> Recall: 0.9647 (avg 5 epochs)\n",
      "[44/243] lr=0.001, batch=32, h1=128, h2=128, dropout=0.3\n",
      "  -> Recall: 0.9588 (avg 7 epochs)\n",
      "[45/243] lr=0.001, batch=32, h1=128, h2=128, dropout=0.5\n",
      "  -> Recall: 0.9706 (avg 7 epochs)\n",
      "[46/243] lr=0.001, batch=32, h1=256, h2=32, dropout=0.2\n",
      "  -> Recall: 0.9706 (avg 4 epochs)\n",
      "[47/243] lr=0.001, batch=32, h1=256, h2=32, dropout=0.3\n",
      "  -> Recall: 0.9765 (avg 4 epochs)\n",
      "[48/243] lr=0.001, batch=32, h1=256, h2=32, dropout=0.5\n",
      "  -> Recall: 0.9647 (avg 4 epochs)\n",
      "[49/243] lr=0.001, batch=32, h1=256, h2=64, dropout=0.2\n",
      "  -> Recall: 0.9706 (avg 5 epochs)\n",
      "[50/243] lr=0.001, batch=32, h1=256, h2=64, dropout=0.3\n",
      "  -> Recall: 0.9706 (avg 3 epochs)\n",
      "[51/243] lr=0.001, batch=32, h1=256, h2=64, dropout=0.5\n",
      "  -> Recall: 0.9647 (avg 6 epochs)\n",
      "[52/243] lr=0.001, batch=32, h1=256, h2=128, dropout=0.2\n",
      "  -> Recall: 0.9647 (avg 4 epochs)\n",
      "[53/243] lr=0.001, batch=32, h1=256, h2=128, dropout=0.3\n",
      "  -> Recall: 0.9706 (avg 4 epochs)\n",
      "[54/243] lr=0.001, batch=32, h1=256, h2=128, dropout=0.5\n",
      "  -> Recall: 0.9706 (avg 5 epochs)\n",
      "[55/243] lr=0.001, batch=64, h1=64, h2=32, dropout=0.2\n",
      "  -> Recall: 0.9529 (avg 4 epochs)\n",
      "[56/243] lr=0.001, batch=64, h1=64, h2=32, dropout=0.3\n",
      "  -> Recall: 0.9824 (avg 3 epochs)\n",
      "[57/243] lr=0.001, batch=64, h1=64, h2=32, dropout=0.5\n",
      "  -> Recall: 0.9412 (avg 7 epochs)\n",
      "[58/243] lr=0.001, batch=64, h1=64, h2=64, dropout=0.2\n",
      "  -> Recall: 0.9529 (avg 6 epochs)\n",
      "[59/243] lr=0.001, batch=64, h1=64, h2=64, dropout=0.3\n",
      "  -> Recall: 0.9588 (avg 9 epochs)\n",
      "[60/243] lr=0.001, batch=64, h1=64, h2=64, dropout=0.5\n",
      "  -> Recall: 0.9765 (avg 2 epochs)\n",
      "[61/243] lr=0.001, batch=64, h1=64, h2=128, dropout=0.2\n",
      "  -> Recall: 0.9647 (avg 6 epochs)\n",
      "[62/243] lr=0.001, batch=64, h1=64, h2=128, dropout=0.3\n",
      "  -> Recall: 0.9529 (avg 4 epochs)\n",
      "[63/243] lr=0.001, batch=64, h1=64, h2=128, dropout=0.5\n",
      "  -> Recall: 0.9529 (avg 8 epochs)\n",
      "[64/243] lr=0.001, batch=64, h1=128, h2=32, dropout=0.2\n",
      "  -> Recall: 0.9765 (avg 1 epochs)\n",
      "[65/243] lr=0.001, batch=64, h1=128, h2=32, dropout=0.3\n",
      "  -> Recall: 0.9588 (avg 6 epochs)\n",
      "[66/243] lr=0.001, batch=64, h1=128, h2=32, dropout=0.5\n",
      "  -> Recall: 0.9647 (avg 12 epochs)\n",
      "[67/243] lr=0.001, batch=64, h1=128, h2=64, dropout=0.2\n",
      "  -> Recall: 0.9706 (avg 3 epochs)\n",
      "[68/243] lr=0.001, batch=64, h1=128, h2=64, dropout=0.3\n",
      "  -> Recall: 0.9529 (avg 5 epochs)\n",
      "[69/243] lr=0.001, batch=64, h1=128, h2=64, dropout=0.5\n",
      "  -> Recall: 0.9588 (avg 5 epochs)\n",
      "[70/243] lr=0.001, batch=64, h1=128, h2=128, dropout=0.2\n",
      "  -> Recall: 0.9706 (avg 6 epochs)\n",
      "[71/243] lr=0.001, batch=64, h1=128, h2=128, dropout=0.3\n",
      "  -> Recall: 0.9706 (avg 3 epochs)\n",
      "[72/243] lr=0.001, batch=64, h1=128, h2=128, dropout=0.5\n",
      "  -> Recall: 0.9588 (avg 5 epochs)\n",
      "[73/243] lr=0.001, batch=64, h1=256, h2=32, dropout=0.2\n",
      "  -> Recall: 0.9647 (avg 3 epochs)\n",
      "[74/243] lr=0.001, batch=64, h1=256, h2=32, dropout=0.3\n",
      "  -> Recall: 0.9706 (avg 4 epochs)\n",
      "[75/243] lr=0.001, batch=64, h1=256, h2=32, dropout=0.5\n",
      "  -> Recall: 0.9647 (avg 7 epochs)\n",
      "[76/243] lr=0.001, batch=64, h1=256, h2=64, dropout=0.2\n",
      "  -> Recall: 0.9706 (avg 4 epochs)\n",
      "[77/243] lr=0.001, batch=64, h1=256, h2=64, dropout=0.3\n",
      "  -> Recall: 0.9706 (avg 4 epochs)\n",
      "[78/243] lr=0.001, batch=64, h1=256, h2=64, dropout=0.5\n",
      "  -> Recall: 0.9647 (avg 2 epochs)\n",
      "[79/243] lr=0.001, batch=64, h1=256, h2=128, dropout=0.2\n",
      "  -> Recall: 0.9706 (avg 4 epochs)\n",
      "[80/243] lr=0.001, batch=64, h1=256, h2=128, dropout=0.3\n",
      "  -> Recall: 0.9647 (avg 7 epochs)\n",
      "[81/243] lr=0.001, batch=64, h1=256, h2=128, dropout=0.5\n",
      "  -> Recall: 0.9706 (avg 3 epochs)\n",
      "[82/243] lr=0.01, batch=16, h1=64, h2=32, dropout=0.2\n",
      "  -> Recall: 0.9706 (avg 3 epochs)\n",
      "[83/243] lr=0.01, batch=16, h1=64, h2=32, dropout=0.3\n",
      "  -> Recall: 0.9588 (avg 2 epochs)\n",
      "[84/243] lr=0.01, batch=16, h1=64, h2=32, dropout=0.5\n",
      "  -> Recall: 0.9765 (avg 5 epochs)\n",
      "[85/243] lr=0.01, batch=16, h1=64, h2=64, dropout=0.2\n",
      "  -> Recall: 0.9824 (avg 3 epochs)\n",
      "[86/243] lr=0.01, batch=16, h1=64, h2=64, dropout=0.3\n",
      "  -> Recall: 0.9824 (avg 5 epochs)\n",
      "[87/243] lr=0.01, batch=16, h1=64, h2=64, dropout=0.5\n",
      "  -> Recall: 0.9765 (avg 4 epochs)\n",
      "[88/243] lr=0.01, batch=16, h1=64, h2=128, dropout=0.2\n",
      "  -> Recall: 0.9647 (avg 3 epochs)\n",
      "[89/243] lr=0.01, batch=16, h1=64, h2=128, dropout=0.3\n",
      "  -> Recall: 0.9706 (avg 3 epochs)\n",
      "[90/243] lr=0.01, batch=16, h1=64, h2=128, dropout=0.5\n",
      "  -> Recall: 0.9706 (avg 3 epochs)\n",
      "[91/243] lr=0.01, batch=16, h1=128, h2=32, dropout=0.2\n",
      "  -> Recall: 0.9765 (avg 5 epochs)\n",
      "[92/243] lr=0.01, batch=16, h1=128, h2=32, dropout=0.3\n",
      "  -> Recall: 0.9706 (avg 4 epochs)\n",
      "[93/243] lr=0.01, batch=16, h1=128, h2=32, dropout=0.5\n",
      "  -> Recall: 0.9765 (avg 8 epochs)\n",
      "[94/243] lr=0.01, batch=16, h1=128, h2=64, dropout=0.2\n",
      "  -> Recall: 0.9588 (avg 1 epochs)\n",
      "[95/243] lr=0.01, batch=16, h1=128, h2=64, dropout=0.3\n",
      "  -> Recall: 0.9706 (avg 5 epochs)\n",
      "[96/243] lr=0.01, batch=16, h1=128, h2=64, dropout=0.5\n",
      "  -> Recall: 0.9706 (avg 4 epochs)\n",
      "[97/243] lr=0.01, batch=16, h1=128, h2=128, dropout=0.2\n",
      "  -> Recall: 0.9765 (avg 3 epochs)\n",
      "[98/243] lr=0.01, batch=16, h1=128, h2=128, dropout=0.3\n",
      "  -> Recall: 0.9706 (avg 5 epochs)\n",
      "[99/243] lr=0.01, batch=16, h1=128, h2=128, dropout=0.5\n",
      "  -> Recall: 0.9824 (avg 6 epochs)\n",
      "[100/243] lr=0.01, batch=16, h1=256, h2=32, dropout=0.2\n",
      "  -> Recall: 0.9706 (avg 5 epochs)\n",
      "[101/243] lr=0.01, batch=16, h1=256, h2=32, dropout=0.3\n",
      "  -> Recall: 0.9706 (avg 4 epochs)\n",
      "[102/243] lr=0.01, batch=16, h1=256, h2=32, dropout=0.5\n",
      "  -> Recall: 0.9706 (avg 3 epochs)\n",
      "[103/243] lr=0.01, batch=16, h1=256, h2=64, dropout=0.2\n",
      "  -> Recall: 0.9647 (avg 3 epochs)\n",
      "[104/243] lr=0.01, batch=16, h1=256, h2=64, dropout=0.3\n",
      "  -> Recall: 0.9765 (avg 4 epochs)\n",
      "[105/243] lr=0.01, batch=16, h1=256, h2=64, dropout=0.5\n",
      "  -> Recall: 0.9765 (avg 5 epochs)\n",
      "[106/243] lr=0.01, batch=16, h1=256, h2=128, dropout=0.2\n",
      "  -> Recall: 0.9706 (avg 2 epochs)\n",
      "[107/243] lr=0.01, batch=16, h1=256, h2=128, dropout=0.3\n",
      "  -> Recall: 0.9765 (avg 3 epochs)\n",
      "[108/243] lr=0.01, batch=16, h1=256, h2=128, dropout=0.5\n",
      "  -> Recall: 0.9765 (avg 5 epochs)\n",
      "[109/243] lr=0.01, batch=32, h1=64, h2=32, dropout=0.2\n",
      "  -> Recall: 0.9765 (avg 5 epochs)\n",
      "[110/243] lr=0.01, batch=32, h1=64, h2=32, dropout=0.3\n",
      "  -> Recall: 0.9824 (avg 4 epochs)\n",
      "[111/243] lr=0.01, batch=32, h1=64, h2=32, dropout=0.5\n",
      "  -> Recall: 0.9824 (avg 4 epochs)\n",
      "[112/243] lr=0.01, batch=32, h1=64, h2=64, dropout=0.2\n",
      "  -> Recall: 0.9765 (avg 1 epochs)\n",
      "[113/243] lr=0.01, batch=32, h1=64, h2=64, dropout=0.3\n",
      "  -> Recall: 0.9824 (avg 3 epochs)\n",
      "[114/243] lr=0.01, batch=32, h1=64, h2=64, dropout=0.5\n",
      "  -> Recall: 0.9824 (avg 5 epochs)\n",
      "[115/243] lr=0.01, batch=32, h1=64, h2=128, dropout=0.2\n",
      "  -> Recall: 0.9765 (avg 4 epochs)\n",
      "[116/243] lr=0.01, batch=32, h1=64, h2=128, dropout=0.3\n",
      "  -> Recall: 0.9706 (avg 3 epochs)\n",
      "[117/243] lr=0.01, batch=32, h1=64, h2=128, dropout=0.5\n",
      "  -> Recall: 0.9647 (avg 3 epochs)\n",
      "[118/243] lr=0.01, batch=32, h1=128, h2=32, dropout=0.2\n",
      "  -> Recall: 0.9765 (avg 2 epochs)\n",
      "[119/243] lr=0.01, batch=32, h1=128, h2=32, dropout=0.3\n",
      "  -> Recall: 0.9706 (avg 3 epochs)\n",
      "[120/243] lr=0.01, batch=32, h1=128, h2=32, dropout=0.5\n",
      "  -> Recall: 0.9765 (avg 5 epochs)\n",
      "[121/243] lr=0.01, batch=32, h1=128, h2=64, dropout=0.2\n",
      "  -> Recall: 0.9706 (avg 3 epochs)\n",
      "[122/243] lr=0.01, batch=32, h1=128, h2=64, dropout=0.3\n",
      "  -> Recall: 0.9706 (avg 2 epochs)\n",
      "[123/243] lr=0.01, batch=32, h1=128, h2=64, dropout=0.5\n",
      "  -> Recall: 0.9706 (avg 4 epochs)\n",
      "[124/243] lr=0.01, batch=32, h1=128, h2=128, dropout=0.2\n",
      "  -> Recall: 0.9706 (avg 3 epochs)\n",
      "[125/243] lr=0.01, batch=32, h1=128, h2=128, dropout=0.3\n",
      "  -> Recall: 0.9824 (avg 5 epochs)\n",
      "[126/243] lr=0.01, batch=32, h1=128, h2=128, dropout=0.5\n",
      "  -> Recall: 0.9765 (avg 4 epochs)\n",
      "[127/243] lr=0.01, batch=32, h1=256, h2=32, dropout=0.2\n",
      "  -> Recall: 0.9765 (avg 7 epochs)\n",
      "[128/243] lr=0.01, batch=32, h1=256, h2=32, dropout=0.3\n",
      "  -> Recall: 0.9706 (avg 2 epochs)\n",
      "[129/243] lr=0.01, batch=32, h1=256, h2=32, dropout=0.5\n",
      "  -> Recall: 0.9706 (avg 3 epochs)\n",
      "[130/243] lr=0.01, batch=32, h1=256, h2=64, dropout=0.2\n",
      "  -> Recall: 0.9824 (avg 5 epochs)\n",
      "[131/243] lr=0.01, batch=32, h1=256, h2=64, dropout=0.3\n",
      "  -> Recall: 0.9706 (avg 3 epochs)\n",
      "[132/243] lr=0.01, batch=32, h1=256, h2=64, dropout=0.5\n",
      "  -> Recall: 0.9706 (avg 5 epochs)\n",
      "[133/243] lr=0.01, batch=32, h1=256, h2=128, dropout=0.2\n",
      "  -> Recall: 0.9706 (avg 4 epochs)\n",
      "[134/243] lr=0.01, batch=32, h1=256, h2=128, dropout=0.3\n",
      "  -> Recall: 0.9706 (avg 1 epochs)\n",
      "[135/243] lr=0.01, batch=32, h1=256, h2=128, dropout=0.5\n",
      "  -> Recall: 0.9765 (avg 3 epochs)\n",
      "[136/243] lr=0.01, batch=64, h1=64, h2=32, dropout=0.2\n",
      "  -> Recall: 0.9706 (avg 5 epochs)\n",
      "[137/243] lr=0.01, batch=64, h1=64, h2=32, dropout=0.3\n",
      "  -> Recall: 0.9647 (avg 3 epochs)\n",
      "[138/243] lr=0.01, batch=64, h1=64, h2=32, dropout=0.5\n",
      "  -> Recall: 0.9588 (avg 3 epochs)\n",
      "[139/243] lr=0.01, batch=64, h1=64, h2=64, dropout=0.2\n",
      "  -> Recall: 0.9765 (avg 3 epochs)\n",
      "[140/243] lr=0.01, batch=64, h1=64, h2=64, dropout=0.3\n",
      "  -> Recall: 0.9706 (avg 4 epochs)\n",
      "[141/243] lr=0.01, batch=64, h1=64, h2=64, dropout=0.5\n",
      "  -> Recall: 0.9647 (avg 4 epochs)\n",
      "[142/243] lr=0.01, batch=64, h1=64, h2=128, dropout=0.2\n",
      "  -> Recall: 0.9647 (avg 2 epochs)\n",
      "[143/243] lr=0.01, batch=64, h1=64, h2=128, dropout=0.3\n",
      "  -> Recall: 0.9765 (avg 3 epochs)\n",
      "[144/243] lr=0.01, batch=64, h1=64, h2=128, dropout=0.5\n",
      "  -> Recall: 0.9765 (avg 3 epochs)\n",
      "[145/243] lr=0.01, batch=64, h1=128, h2=32, dropout=0.2\n",
      "  -> Recall: 0.9765 (avg 2 epochs)\n",
      "[146/243] lr=0.01, batch=64, h1=128, h2=32, dropout=0.3\n",
      "  -> Recall: 0.9824 (avg 5 epochs)\n",
      "[147/243] lr=0.01, batch=64, h1=128, h2=32, dropout=0.5\n",
      "  -> Recall: 0.9647 (avg 3 epochs)\n",
      "[148/243] lr=0.01, batch=64, h1=128, h2=64, dropout=0.2\n",
      "  -> Recall: 0.9765 (avg 1 epochs)\n",
      "[149/243] lr=0.01, batch=64, h1=128, h2=64, dropout=0.3\n",
      "  -> Recall: 0.9706 (avg 3 epochs)\n",
      "[150/243] lr=0.01, batch=64, h1=128, h2=64, dropout=0.5\n",
      "  -> Recall: 0.9706 (avg 1 epochs)\n",
      "[151/243] lr=0.01, batch=64, h1=128, h2=128, dropout=0.2\n",
      "  -> Recall: 0.9706 (avg 3 epochs)\n",
      "[152/243] lr=0.01, batch=64, h1=128, h2=128, dropout=0.3\n",
      "  -> Recall: 0.9765 (avg 2 epochs)\n",
      "[153/243] lr=0.01, batch=64, h1=128, h2=128, dropout=0.5\n",
      "  -> Recall: 0.9824 (avg 4 epochs)\n",
      "[154/243] lr=0.01, batch=64, h1=256, h2=32, dropout=0.2\n",
      "  -> Recall: 0.9824 (avg 5 epochs)\n",
      "[155/243] lr=0.01, batch=64, h1=256, h2=32, dropout=0.3\n",
      "  -> Recall: 0.9765 (avg 4 epochs)\n",
      "[156/243] lr=0.01, batch=64, h1=256, h2=32, dropout=0.5\n",
      "  -> Recall: 0.9765 (avg 6 epochs)\n",
      "[157/243] lr=0.01, batch=64, h1=256, h2=64, dropout=0.2\n",
      "  -> Recall: 0.9765 (avg 4 epochs)\n",
      "[158/243] lr=0.01, batch=64, h1=256, h2=64, dropout=0.3\n",
      "  -> Recall: 0.9706 (avg 4 epochs)\n",
      "[159/243] lr=0.01, batch=64, h1=256, h2=64, dropout=0.5\n",
      "  -> Recall: 0.9647 (avg 3 epochs)\n",
      "[160/243] lr=0.01, batch=64, h1=256, h2=128, dropout=0.2\n",
      "  -> Recall: 0.9765 (avg 3 epochs)\n",
      "[161/243] lr=0.01, batch=64, h1=256, h2=128, dropout=0.3\n",
      "  -> Recall: 0.9706 (avg 2 epochs)\n",
      "[162/243] lr=0.01, batch=64, h1=256, h2=128, dropout=0.5\n",
      "  -> Recall: 0.9706 (avg 2 epochs)\n",
      "[163/243] lr=0.1, batch=16, h1=64, h2=32, dropout=0.2\n",
      "  -> Recall: 0.9706 (avg 6 epochs)\n",
      "[164/243] lr=0.1, batch=16, h1=64, h2=32, dropout=0.3\n",
      "  -> Recall: 0.9765 (avg 2 epochs)\n",
      "[165/243] lr=0.1, batch=16, h1=64, h2=32, dropout=0.5\n",
      "  -> Recall: 0.9765 (avg 3 epochs)\n",
      "[166/243] lr=0.1, batch=16, h1=64, h2=64, dropout=0.2\n",
      "  -> Recall: 0.9706 (avg 3 epochs)\n",
      "[167/243] lr=0.1, batch=16, h1=64, h2=64, dropout=0.3\n",
      "  -> Recall: 0.9765 (avg 9 epochs)\n",
      "[168/243] lr=0.1, batch=16, h1=64, h2=64, dropout=0.5\n",
      "  -> Recall: 0.9706 (avg 4 epochs)\n",
      "[169/243] lr=0.1, batch=16, h1=64, h2=128, dropout=0.2\n",
      "  -> Recall: 0.9706 (avg 3 epochs)\n",
      "[170/243] lr=0.1, batch=16, h1=64, h2=128, dropout=0.3\n",
      "  -> Recall: 0.9647 (avg 3 epochs)\n",
      "[171/243] lr=0.1, batch=16, h1=64, h2=128, dropout=0.5\n",
      "  -> Recall: 0.9706 (avg 2 epochs)\n",
      "[172/243] lr=0.1, batch=16, h1=128, h2=32, dropout=0.2\n",
      "  -> Recall: 0.9706 (avg 3 epochs)\n",
      "[173/243] lr=0.1, batch=16, h1=128, h2=32, dropout=0.3\n",
      "  -> Recall: 0.9647 (avg 6 epochs)\n",
      "[174/243] lr=0.1, batch=16, h1=128, h2=32, dropout=0.5\n",
      "  -> Recall: 0.9824 (avg 5 epochs)\n",
      "[175/243] lr=0.1, batch=16, h1=128, h2=64, dropout=0.2\n",
      "  -> Recall: 0.9824 (avg 5 epochs)\n",
      "[176/243] lr=0.1, batch=16, h1=128, h2=64, dropout=0.3\n",
      "  -> Recall: 0.9824 (avg 8 epochs)\n",
      "[177/243] lr=0.1, batch=16, h1=128, h2=64, dropout=0.5\n",
      "  -> Recall: 0.9647 (avg 3 epochs)\n",
      "[178/243] lr=0.1, batch=16, h1=128, h2=128, dropout=0.2\n",
      "  -> Recall: 0.9765 (avg 4 epochs)\n",
      "[179/243] lr=0.1, batch=16, h1=128, h2=128, dropout=0.3\n",
      "  -> Recall: 0.9706 (avg 3 epochs)\n",
      "[180/243] lr=0.1, batch=16, h1=128, h2=128, dropout=0.5\n",
      "  -> Recall: 0.9647 (avg 5 epochs)\n",
      "[181/243] lr=0.1, batch=16, h1=256, h2=32, dropout=0.2\n",
      "  -> Recall: 0.9647 (avg 2 epochs)\n",
      "[182/243] lr=0.1, batch=16, h1=256, h2=32, dropout=0.3\n",
      "  -> Recall: 0.9706 (avg 4 epochs)\n",
      "[183/243] lr=0.1, batch=16, h1=256, h2=32, dropout=0.5\n",
      "  -> Recall: 0.9588 (avg 3 epochs)\n",
      "[184/243] lr=0.1, batch=16, h1=256, h2=64, dropout=0.2\n",
      "  -> Recall: 0.9706 (avg 6 epochs)\n",
      "[185/243] lr=0.1, batch=16, h1=256, h2=64, dropout=0.3\n",
      "  -> Recall: 0.9706 (avg 2 epochs)\n",
      "[186/243] lr=0.1, batch=16, h1=256, h2=64, dropout=0.5\n",
      "  -> Recall: 0.9706 (avg 4 epochs)\n",
      "[187/243] lr=0.1, batch=16, h1=256, h2=128, dropout=0.2\n",
      "  -> Recall: 0.9706 (avg 4 epochs)\n",
      "[188/243] lr=0.1, batch=16, h1=256, h2=128, dropout=0.3\n",
      "  -> Recall: 0.9706 (avg 5 epochs)\n",
      "[189/243] lr=0.1, batch=16, h1=256, h2=128, dropout=0.5\n",
      "  -> Recall: 0.9588 (avg 5 epochs)\n",
      "[190/243] lr=0.1, batch=32, h1=64, h2=32, dropout=0.2\n",
      "  -> Recall: 0.9765 (avg 2 epochs)\n",
      "[191/243] lr=0.1, batch=32, h1=64, h2=32, dropout=0.3\n",
      "  -> Recall: 0.9765 (avg 4 epochs)\n",
      "[192/243] lr=0.1, batch=32, h1=64, h2=32, dropout=0.5\n",
      "  -> Recall: 0.9706 (avg 2 epochs)\n",
      "[193/243] lr=0.1, batch=32, h1=64, h2=64, dropout=0.2\n",
      "  -> Recall: 0.9706 (avg 1 epochs)\n",
      "[194/243] lr=0.1, batch=32, h1=64, h2=64, dropout=0.3\n",
      "  -> Recall: 0.9706 (avg 2 epochs)\n",
      "[195/243] lr=0.1, batch=32, h1=64, h2=64, dropout=0.5\n",
      "  -> Recall: 0.9706 (avg 1 epochs)\n",
      "[196/243] lr=0.1, batch=32, h1=64, h2=128, dropout=0.2\n",
      "  -> Recall: 0.9706 (avg 3 epochs)\n",
      "[197/243] lr=0.1, batch=32, h1=64, h2=128, dropout=0.3\n",
      "  -> Recall: 0.9824 (avg 7 epochs)\n",
      "[198/243] lr=0.1, batch=32, h1=64, h2=128, dropout=0.5\n",
      "  -> Recall: 0.9765 (avg 1 epochs)\n",
      "[199/243] lr=0.1, batch=32, h1=128, h2=32, dropout=0.2\n",
      "  -> Recall: 0.9765 (avg 2 epochs)\n",
      "[200/243] lr=0.1, batch=32, h1=128, h2=32, dropout=0.3\n",
      "  -> Recall: 0.9706 (avg 4 epochs)\n",
      "[201/243] lr=0.1, batch=32, h1=128, h2=32, dropout=0.5\n",
      "  -> Recall: 0.9588 (avg 1 epochs)\n",
      "[202/243] lr=0.1, batch=32, h1=128, h2=64, dropout=0.2\n",
      "  -> Recall: 0.9706 (avg 3 epochs)\n",
      "[203/243] lr=0.1, batch=32, h1=128, h2=64, dropout=0.3\n",
      "  -> Recall: 0.9706 (avg 3 epochs)\n",
      "[204/243] lr=0.1, batch=32, h1=128, h2=64, dropout=0.5\n",
      "  -> Recall: 0.9765 (avg 3 epochs)\n",
      "[205/243] lr=0.1, batch=32, h1=128, h2=128, dropout=0.2\n",
      "  -> Recall: 0.9765 (avg 4 epochs)\n",
      "[206/243] lr=0.1, batch=32, h1=128, h2=128, dropout=0.3\n",
      "  -> Recall: 0.9706 (avg 4 epochs)\n",
      "[207/243] lr=0.1, batch=32, h1=128, h2=128, dropout=0.5\n",
      "  -> Recall: 0.9765 (avg 2 epochs)\n",
      "[208/243] lr=0.1, batch=32, h1=256, h2=32, dropout=0.2\n",
      "  -> Recall: 0.9647 (avg 2 epochs)\n",
      "[209/243] lr=0.1, batch=32, h1=256, h2=32, dropout=0.3\n",
      "  -> Recall: 0.9706 (avg 3 epochs)\n",
      "[210/243] lr=0.1, batch=32, h1=256, h2=32, dropout=0.5\n",
      "  -> Recall: 0.9765 (avg 3 epochs)\n",
      "[211/243] lr=0.1, batch=32, h1=256, h2=64, dropout=0.2\n",
      "  -> Recall: 0.9706 (avg 2 epochs)\n",
      "[212/243] lr=0.1, batch=32, h1=256, h2=64, dropout=0.3\n",
      "  -> Recall: 0.9706 (avg 3 epochs)\n",
      "[213/243] lr=0.1, batch=32, h1=256, h2=64, dropout=0.5\n",
      "  -> Recall: 0.9765 (avg 2 epochs)\n",
      "[214/243] lr=0.1, batch=32, h1=256, h2=128, dropout=0.2\n",
      "  -> Recall: 0.9706 (avg 1 epochs)\n",
      "[215/243] lr=0.1, batch=32, h1=256, h2=128, dropout=0.3\n",
      "  -> Recall: 0.9706 (avg 2 epochs)\n",
      "[216/243] lr=0.1, batch=32, h1=256, h2=128, dropout=0.5\n",
      "  -> Recall: 0.9706 (avg 3 epochs)\n",
      "[217/243] lr=0.1, batch=64, h1=64, h2=32, dropout=0.2\n",
      "  -> Recall: 0.9765 (avg 1 epochs)\n",
      "[218/243] lr=0.1, batch=64, h1=64, h2=32, dropout=0.3\n",
      "  -> Recall: 0.9765 (avg 3 epochs)\n",
      "[219/243] lr=0.1, batch=64, h1=64, h2=32, dropout=0.5\n",
      "  -> Recall: 0.9706 (avg 3 epochs)\n",
      "[220/243] lr=0.1, batch=64, h1=64, h2=64, dropout=0.2\n",
      "  -> Recall: 0.9706 (avg 1 epochs)\n",
      "[221/243] lr=0.1, batch=64, h1=64, h2=64, dropout=0.3\n",
      "  -> Recall: 0.9824 (avg 5 epochs)\n",
      "[222/243] lr=0.1, batch=64, h1=64, h2=64, dropout=0.5\n",
      "  -> Recall: 0.9765 (avg 2 epochs)\n",
      "[223/243] lr=0.1, batch=64, h1=64, h2=128, dropout=0.2\n",
      "  -> Recall: 0.9706 (avg 1 epochs)\n",
      "[224/243] lr=0.1, batch=64, h1=64, h2=128, dropout=0.3\n",
      "  -> Recall: 0.9647 (avg 1 epochs)\n",
      "[225/243] lr=0.1, batch=64, h1=64, h2=128, dropout=0.5\n",
      "  -> Recall: 0.9824 (avg 2 epochs)\n",
      "[226/243] lr=0.1, batch=64, h1=128, h2=32, dropout=0.2\n",
      "  -> Recall: 0.9588 (avg 1 epochs)\n",
      "[227/243] lr=0.1, batch=64, h1=128, h2=32, dropout=0.3\n",
      "  -> Recall: 0.9765 (avg 1 epochs)\n",
      "[228/243] lr=0.1, batch=64, h1=128, h2=32, dropout=0.5\n",
      "  -> Recall: 0.9765 (avg 2 epochs)\n",
      "[229/243] lr=0.1, batch=64, h1=128, h2=64, dropout=0.2\n",
      "  -> Recall: 0.9765 (avg 2 epochs)\n",
      "[230/243] lr=0.1, batch=64, h1=128, h2=64, dropout=0.3\n",
      "  -> Recall: 0.9882 (avg 2 epochs)\n",
      "[231/243] lr=0.1, batch=64, h1=128, h2=64, dropout=0.5\n",
      "  -> Recall: 0.9824 (avg 2 epochs)\n",
      "[232/243] lr=0.1, batch=64, h1=128, h2=128, dropout=0.2\n",
      "  -> Recall: 0.9765 (avg 1 epochs)\n",
      "[233/243] lr=0.1, batch=64, h1=128, h2=128, dropout=0.3\n",
      "  -> Recall: 0.9765 (avg 1 epochs)\n",
      "[234/243] lr=0.1, batch=64, h1=128, h2=128, dropout=0.5\n",
      "  -> Recall: 0.9765 (avg 1 epochs)\n",
      "[235/243] lr=0.1, batch=64, h1=256, h2=32, dropout=0.2\n",
      "  -> Recall: 0.9765 (avg 2 epochs)\n",
      "[236/243] lr=0.1, batch=64, h1=256, h2=32, dropout=0.3\n",
      "  -> Recall: 0.9882 (avg 4 epochs)\n",
      "[237/243] lr=0.1, batch=64, h1=256, h2=32, dropout=0.5\n",
      "  -> Recall: 0.9824 (avg 1 epochs)\n",
      "[238/243] lr=0.1, batch=64, h1=256, h2=64, dropout=0.2\n",
      "  -> Recall: 0.9706 (avg 1 epochs)\n",
      "[239/243] lr=0.1, batch=64, h1=256, h2=64, dropout=0.3\n",
      "  -> Recall: 0.9765 (avg 1 epochs)\n",
      "[240/243] lr=0.1, batch=64, h1=256, h2=64, dropout=0.5\n",
      "  -> Recall: 0.9765 (avg 2 epochs)\n",
      "[241/243] lr=0.1, batch=64, h1=256, h2=128, dropout=0.2\n",
      "  -> Recall: 0.9765 (avg 3 epochs)\n",
      "[242/243] lr=0.1, batch=64, h1=256, h2=128, dropout=0.3\n",
      "  -> Recall: 0.9824 (avg 2 epochs)\n",
      "[243/243] lr=0.1, batch=64, h1=256, h2=128, dropout=0.5\n",
      "  -> Recall: 0.9824 (avg 2 epochs)\n",
      "\n",
      "======================================================================\n",
      "BEST HYPERPARAMETERS FROM CROSS-VALIDATION\n",
      "======================================================================\n",
      "Learning Rate:   0.1\n",
      "Batch Size:      64\n",
      "Hidden Layer 1:  128\n",
      "Hidden Layer 2:  64\n",
      "Dropout Rate:    0.3\n",
      "Avg Epochs Used: 1\n",
      "\n",
      "Best CV Recall: 0.9882\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Cross-Validation with Hyperparameter Tuning (with Early Stopping)\n",
    "\n",
    "# Define hyperparameter grid (removed num_epochs - early stopping handles this)\n",
    "param_grid = {\n",
    "    'learning_rate': [0.001, 0.01, 0.1],\n",
    "    'batch_size': [16, 32, 64],\n",
    "    'hidden_size1': [64, 128, 256],\n",
    "    'hidden_size2': [32, 64, 128],\n",
    "    'dropout_rate': [0.2, 0.3, 0.5]\n",
    "}\n",
    "\n",
    "# Early stopping parameters\n",
    "max_epochs = 100  # maximum epochs to train\n",
    "patience = 10     # stop if no improvement for this many epochs\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"HYPERPARAMETER TUNING WITH CROSS-VALIDATION (EARLY STOPPING)\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Parameters to tune:\")\n",
    "for param, values in param_grid.items():\n",
    "    print(f\"  {param}: {values}\")\n",
    "print(f\"\\nEarly Stopping: patience={patience}, max_epochs={max_epochs}\")\n",
    "print(f\"Total combinations: {3*3*3*3*3} = 243\")\n",
    "print(f\"K-folds: 5\")\n",
    "print(f\"Total training runs: 243 x 5 = 1215 (with early stopping)\")\n",
    "print(\"=\" * 70)\n",
    "print()\n",
    "\n",
    "# Initialize StratifiedKFold\n",
    "n_splits = 5\n",
    "skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "# Store results\n",
    "cv_results = []\n",
    "\n",
    "# Convert data to numpy for CV splitting\n",
    "X_train_np = X_train.values\n",
    "y_train_np = y_train.values\n",
    "\n",
    "# Generate all parameter combinations\n",
    "from itertools import product\n",
    "param_combinations = list(product(\n",
    "    param_grid['learning_rate'],\n",
    "    param_grid['batch_size'],\n",
    "    param_grid['hidden_size1'],\n",
    "    param_grid['hidden_size2'],\n",
    "    param_grid['dropout_rate']\n",
    "))\n",
    "\n",
    "# Grid search with cross-validation\n",
    "for idx, (lr, batch_size, h1, h2, dropout) in enumerate(param_combinations, 1):\n",
    "    print(f\"[{idx}/{len(param_combinations)}] lr={lr}, batch={batch_size}, h1={h1}, h2={h2}, dropout={dropout}\")\n",
    "    \n",
    "    fold_recalls = []\n",
    "    fold_epochs_used = []\n",
    "    \n",
    "    # K-Fold Cross-Validation\n",
    "    for fold, (train_idx, val_idx) in enumerate(skf.split(X_train_np, y_train_np), 1):\n",
    "        # Split data for this fold\n",
    "        X_fold_train, X_fold_val = X_train_np[train_idx], X_train_np[val_idx]\n",
    "        y_fold_train, y_fold_val = y_train_np[train_idx], y_train_np[val_idx]\n",
    "        \n",
    "        # CRITICAL: Fit NEW scaler on THIS fold's training data only\n",
    "        fold_scaler = StandardScaler()\n",
    "        X_fold_train_scaled = fold_scaler.fit_transform(X_fold_train)\n",
    "        X_fold_val_scaled = fold_scaler.transform(X_fold_val)\n",
    "        \n",
    "        # Convert to tensors\n",
    "        X_fold_train_tensor = torch.FloatTensor(X_fold_train_scaled)\n",
    "        y_fold_train_tensor = torch.LongTensor(y_fold_train)\n",
    "        X_fold_val_tensor = torch.FloatTensor(X_fold_val_scaled)\n",
    "        y_fold_val_tensor = torch.LongTensor(y_fold_val)\n",
    "        \n",
    "        # Create DataLoader for this fold\n",
    "        fold_train_dataset = TensorDataset(X_fold_train_tensor, y_fold_train_tensor)\n",
    "        fold_train_loader = DataLoader(fold_train_dataset, batch_size=batch_size, shuffle=True)\n",
    "        \n",
    "        # Initialize model\n",
    "        model = BreastCancerNet(\n",
    "            input_size=30,\n",
    "            hidden_size1=h1,\n",
    "            hidden_size2=h2,\n",
    "            dropout_rate=dropout,\n",
    "            num_classes=2\n",
    "        )\n",
    "        \n",
    "        # Define optimizer and loss\n",
    "        optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        \n",
    "        # Early stopping variables\n",
    "        best_recall = 0.0\n",
    "        epochs_without_improvement = 0\n",
    "        best_epoch = 0\n",
    "        \n",
    "        # Train model with early stopping\n",
    "        for epoch in range(max_epochs):\n",
    "            # Training phase\n",
    "            model.train()\n",
    "            for inputs, labels in fold_train_loader:\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            \n",
    "            # Evaluation phase (check for early stopping)\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                outputs = model(X_fold_val_tensor)\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                current_recall = recall_score(y_fold_val_tensor.numpy(), predicted.numpy())\n",
    "            \n",
    "            # Check if recall improved\n",
    "            if current_recall > best_recall:\n",
    "                best_recall = current_recall\n",
    "                best_epoch = epoch + 1\n",
    "                epochs_without_improvement = 0\n",
    "            else:\n",
    "                epochs_without_improvement += 1\n",
    "            \n",
    "            # Early stopping check\n",
    "            if epochs_without_improvement >= patience:\n",
    "                break\n",
    "        \n",
    "        fold_recalls.append(best_recall)\n",
    "        fold_epochs_used.append(best_epoch)\n",
    "    \n",
    "    # Calculate average recall across folds\n",
    "    avg_recall = np.mean(fold_recalls)\n",
    "    std_recall = np.std(fold_recalls)\n",
    "    avg_epochs = np.mean(fold_epochs_used)\n",
    "    \n",
    "    print(f\"  -> Recall: {avg_recall:.4f} (avg {avg_epochs:.0f} epochs)\")\n",
    "    \n",
    "    # Store results\n",
    "    cv_results.append({\n",
    "        'learning_rate': lr,\n",
    "        'batch_size': batch_size,\n",
    "        'hidden_size1': h1,\n",
    "        'hidden_size2': h2,\n",
    "        'dropout_rate': dropout,\n",
    "        'num_epochs': int(avg_epochs),  # average epochs used\n",
    "        'avg_recall': avg_recall,\n",
    "        'std_recall': std_recall\n",
    "    })\n",
    "\n",
    "# Find best hyperparameters\n",
    "best_result = max(cv_results, key=lambda x: x['avg_recall'])\n",
    "\n",
    "print()\n",
    "print(\"=\" * 70)\n",
    "print(\"BEST HYPERPARAMETERS FROM CROSS-VALIDATION\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Learning Rate:   {best_result['learning_rate']}\")\n",
    "print(f\"Batch Size:      {best_result['batch_size']}\")\n",
    "print(f\"Hidden Layer 1:  {best_result['hidden_size1']}\")\n",
    "print(f\"Hidden Layer 2:  {best_result['hidden_size2']}\")\n",
    "print(f\"Dropout Rate:    {best_result['dropout_rate']}\")\n",
    "print(f\"Avg Epochs Used: {best_result['num_epochs']}\")\n",
    "print(f\"\\nBest CV Recall: {best_result['avg_recall']:.4f}\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "453a086e",
   "metadata": {},
   "source": [
    "#### 7. Best Model Results and Final Training\n",
    "\n",
    "Using the best hyperparameters identified through cross-validation, we now train the final optimized model on the complete training dataset. This final model uses the hyperparameter combination that achieved the highest average recall score during cross-validation. Since cross-validation already validated the model internally, we don't perform separate evaluation here. The model is ready for final test set evaluation after comparing with the other three models (SVM, Logistic Regression, XGBoost)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5122dc6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "FINAL MODEL TRAINING WITH BEST HYPERPARAMETERS\n",
      "============================================================\n",
      "  Learning Rate:   0.1\n",
      "  Batch Size:      64\n",
      "  Hidden Layer 1:  128\n",
      "  Hidden Layer 2:  64\n",
      "  Dropout Rate:    0.3\n",
      "  Epochs:          1\n",
      "\n",
      "Best CV Recall Score: 0.9882\n",
      "============================================================\n",
      "FINAL MODEL TRAINING COMPLETE\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Train final model with best hyperparameters\n",
    "\n",
    "# Recreate DataLoader with best batch size\n",
    "best_batch_size = best_result['batch_size']\n",
    "train_loader_tuned = DataLoader(train_dataset, batch_size=best_batch_size, shuffle=True)\n",
    "\n",
    "# Initialize tuned model with best hyperparameters\n",
    "tuned_model = BreastCancerNet(\n",
    "    input_size=30,\n",
    "    hidden_size1=best_result['hidden_size1'],\n",
    "    hidden_size2=best_result['hidden_size2'],\n",
    "    dropout_rate=best_result['dropout_rate'],\n",
    "    num_classes=2\n",
    ")\n",
    "\n",
    "# Define optimizer and loss with best learning rate\n",
    "criterion_tuned = nn.CrossEntropyLoss()\n",
    "optimizer_tuned = optim.Adam(tuned_model.parameters(), lr=best_result['learning_rate'])\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"FINAL MODEL TRAINING WITH BEST HYPERPARAMETERS\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"  Learning Rate:   {best_result['learning_rate']}\")\n",
    "print(f\"  Batch Size:      {best_result['batch_size']}\")\n",
    "print(f\"  Hidden Layer 1:  {best_result['hidden_size1']}\")\n",
    "print(f\"  Hidden Layer 2:  {best_result['hidden_size2']}\")\n",
    "print(f\"  Dropout Rate:    {best_result['dropout_rate']}\")\n",
    "print(f\"  Epochs:          {best_result['num_epochs']}\")\n",
    "print(f\"\\nBest CV Recall Score: {best_result['avg_recall']:.4f}\")\n",
    "\n",
    "# Train the tuned model\n",
    "tuned_train_losses = train_model(\n",
    "    model=tuned_model,\n",
    "    train_loader=train_loader_tuned,\n",
    "    criterion=criterion_tuned,\n",
    "    optimizer=optimizer_tuned,\n",
    "    num_epochs=best_result['num_epochs']\n",
    ")\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"FINAL MODEL TRAINING COMPLETE\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5af2c54",
   "metadata": {},
   "source": [
    "#### 8. Summary and Model Selection\n",
    "\n",
    "After comprehensive experimentation with feedforward neural networks for breast cancer classification, we implemented a multilayer perceptron with rigorous hyperparameter optimization. Using our training dataset of 455 samples, we performed 5-fold StratifiedKFold cross-validation testing 243 hyperparameter combinations (1,215 training runs) across five key parameters: learning rate, batch size, hidden layer sizes, and dropout rate. Each training run incorporated early stopping with a patience of 10 epochs, which automatically halted training when recall stopped improvingâ€”preventing overfitting and significantly reducing computation time. The grid search optimized for recall to minimize false negatives (missed cancers), which is critical in medical diagnosis. The best model achieved a cross-validation recall score of 0.9882 with optimal hyperparameters: learning rate=0.1, batch size=64, hidden layers=[128, 64], and dropout=0.3, converging in just 1 epoch on average. For each cross-validation fold, we fit a NEW StandardScaler on the training portion only, satisfying the preprocessing-in-pipeline requirement and preventing data leakage. This optimized feedforward neural network represents our neural network contribution and will be evaluated on the held-out test set in comparison with Support Vector Machine, Logistic Regression, and XGBoost models to determine the best overall classifier for deployment in breast cancer diagnosis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f02ee82",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1abd9022",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model, scaler, and parameters saved to Output folder!\n",
      "  - best_model.pth\n",
      "  - scaler.pkl\n",
      "  - best_params.json\n"
     ]
    }
   ],
   "source": [
    "# Saving the trained model and scaler for use in test_evaluation notebook\n",
    "import joblib\n",
    "\n",
    "# Saving the model weights\n",
    "torch.save(tuned_model.state_dict(), '../Output/best_model.pth')\n",
    "\n",
    "# Saving the scaler\n",
    "joblib.dump(scaler, '../Output/scaler.pkl')\n",
    "\n",
    "# Saving best params for reference\n",
    "import json\n",
    "with open('../Output/best_params.json', 'w') as f:\n",
    "    json.dump(best_result, f)\n",
    "\n",
    "print(\"Model, scaler, and parameters saved to Output folder!\")\n",
    "print(\"  - best_model.pth\")\n",
    "print(\"  - scaler.pkl\") \n",
    "print(\"  - best_params.json\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
